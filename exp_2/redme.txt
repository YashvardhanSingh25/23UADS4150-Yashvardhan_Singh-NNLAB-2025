# XOR MLP Implementation (Without Backpropagation)

## Overview
This project implements a simple Multi-Layer Perceptron (MLP) to learn the XOR Boolean function using NumPy. Instead of backpropagation, it employs a **random search** approach to find the best weights and biases that minimize the loss function.

## How It Works
### 1. Data Preparation
The XOR function is a fundamental problem in neural networks because it is not linearly separable. The input and expected outputs are:

| Input | Output |
|--------|--------|
| (0,0)  | 0      |
| (0,1)  | 1      |
| (1,0)  | 1      |
| (1,1)  | 0      |

### 2. Neural Network Architecture
- **Input Layer**: 2 neurons (for two binary inputs)
- **Hidden Layer**: 2 neurons
- **Output Layer**: 1 neuron
- **Activation Function**: Step function (instead of sigmoid)

### 3. Training Process
- The model initializes random weights and biases.
- A **random search** method is used to find the best parameters by generating random weight configurations and evaluating their performance.
- The Mean Squared Error (MSE) loss is computed.
- The best configuration (with the lowest loss) is selected.

### 4. Prediction
- The trained model is used to predict XOR outputs by performing a forward pass with the best weights and biases.

## Code Explanation
1. **Activation Function**: The step function is used to determine neuron activations.
2. **Training Function (`train_xor_mlp`)**:
   - Iterates for a defined number of epochs.
   - Generates random weights and biases.
   - Computes forward propagation.
   - Evaluates the loss and stores the best-performing weights.
3. **Prediction Function (`predict_xor_mlp`)**:
   - Uses the best weights to make predictions on new inputs.

## Running the Code
Ensure you have NumPy installed. Run the Python script to train the model and test its accuracy on the XOR function.
```sh
python xor_mlp.py
```

## Expected Output
After training, the model should produce predictions close to:
```
Predictions: [0 1 1 0]
```
This confirms that the MLP has successfully learned the XOR function.

## Limitations
- The model relies on **random search**, which may not be as efficient as backpropagation-based learning.
- The step function prevents gradient-based optimization methods from being used.

## Conclusion
This implementation demonstrates how an MLP can learn the XOR function without backpropagation, relying on brute-force search instead. While not optimal, it showcases an alternative approach to training neural networks.

