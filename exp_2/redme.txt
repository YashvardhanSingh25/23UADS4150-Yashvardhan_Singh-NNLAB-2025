# Multi-Layer Perceptron (MLP) for XOR Function

## Introduction
This project is about teaching a computer to learn the XOR function using a simple neural network called a **Multi-Layer Perceptron (MLP)**. XOR is a logic operation that follows these rules:

- **0 XOR 0 = 0**
- **0 XOR 1 = 1**
- **1 XOR 0 = 1**
- **1 XOR 1 = 0**

Since XOR is not linearly separable (meaning a straight line cannot split its true and false values), we need a hidden layer in our neural network to solve it.

## How It Works
Our neural network consists of:
- **2 input neurons** (one for each input value)
- **2 hidden neurons** (to help learn patterns)
- **1 output neuron** (to give the final answer: 0 or 1)

We use a **step function** as the activation function, which means that the network outputs either 0 or 1 based on whether a value is greater than or equal to 0.

## Code Explanation
### 1. Import Libraries
```python
import numpy as np
```
We use NumPy for mathematical operations.

### 2. Define the Step Activation Function
```python
def step_function(x):
    return np.where(x >= 0, 1, 0)
```
This function returns `1` if the input is `>= 0` and `0` otherwise.

### 3. Train the Neural Network
```python
def train_xor_mlp(epochs=10000, learning_rate=0.1):
```
- **epochs=10000**: The number of times the network learns from the data.
- **learning_rate=0.1**: Determines how much the network updates its knowledge each step.

#### a. Define the XOR Input and Output
```python
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])
```
These are the four possible inputs and their correct outputs.

#### b. Initialize Weights and Biases
We randomly assign values to weights and biases at the beginning.

#### c. Forward Pass (Prediction Step)
Each layer processes the input using matrix multiplication and passes it forward.

#### d. Backpropagation (Learning Step)
The error is calculated, and the network updates its weights to improve performance.

### 4. Make Predictions
```python
def predict_xor_mlp(X, weights_input_hidden, bias_hidden, weights_hidden_output, bias_output):
```
This function takes new inputs and predicts their outputs using the trained network.

### 5. Train and Test the Network
```python
weights_input_hidden, bias_hidden, weights_hidden_output, bias_output = train_xor_mlp()
X_test = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y_pred = predict_xor_mlp(X_test, weights_input_hidden, bias_hidden, weights_hidden_output, bias_output)
print("Predictions:", y_pred.flatten())
```
This part trains the model and prints the predictions for all XOR inputs.

## Expected Output
The correct XOR output is:
```
Predictions: [0 1 1 0]
```
However, because we use a **step function**, the network may not always learn correctly. A better activation function like **sigmoid** could improve accuracy.

## Conclusion
This project shows how a simple neural network with a hidden layer can learn the XOR function. Although step functions make learning difficult, using **sigmoid** or **ReLU** activations could improve results.

## Possible Improvements
- Use a **sigmoid** activation function instead of a step function.
- Try a different number of hidden neurons to see how it affects learning.
- Experiment with different learning rates and epochs to improve accuracy.

