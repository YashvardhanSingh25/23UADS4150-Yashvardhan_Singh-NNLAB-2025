# README: Implementing Perceptron Learning with Gradient Descent in Python



## What is Gradient Descent?
Gradient Descent is a way for the perceptron to improve its learning. It gradually adjusts the weights so that mistakes become smaller over time.

Think of it as learning how to ride a bikeâ€”you start wobbly but get better as you practice!

---

## How Does This Work?
1. **Takes Inputs**: The perceptron receives inputs (like height and weight).
2. **Applies Weights**: It multiplies each input by a weight (importance value).
3. **Calculates the Error**: It compares its prediction with the correct answer.
4. **Adjusts Weights Using Gradient Descent**: It updates weights slightly to reduce mistakes.
5. **Repeats Until Learning is Complete**: It keeps adjusting until it predicts well.

---

## Code Explanation

### Step 1: Import Required Libraries
We use NumPy for number calculations and Pandas for handling data.
```python
import numpy as np
import pandas as pd
```

### Step 2: Define the Perceptron Class
This class stores:
- **Weights** (which change over time)
- **Learning Rate** (how fast it learns)
- **Epochs** (how many times it trains on data)
- **Activation Function** (how it decides outputs)

```python
class PerceptronGD:
    def __init__(self, input_size, learning_rate=0.01, epochs=100, activation='linear'):
        self.weights = np.zeros(input_size + 1)  # +1 for bias
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.activation = activation
```

### Step 3: Activation Function
This function decides if an output should be **ON (1)** or **OFF (0)**.
```python
    def activation_function(self, x):
        if self.activation == 'linear':
            return x
        elif self.activation == 'sigmoid':
            return 1 / (1 + np.exp(-x))
        else:
            return 1 if x >= 0 else 0
```

### Step 4: Prediction Function
This function takes an input, calculates the weighted sum, and applies the activation function.
```python
    def predict(self, x):
        x = np.insert(x, 0, 1)  # Insert bias term
        return self.activation_function(np.dot(self.weights, x))
```

### Step 5: Training Function (Using Gradient Descent!)
The perceptron learns by adjusting weights based on errors.
```python
    def train(self, X, y):
        X = np.insert(X, 0, 1, axis=1)  # Insert bias term
        for _ in range(self.epochs):
            gradient = np.zeros_like(self.weights)
            for i in range(len(y)):
                prediction = self.activation_function(np.dot(self.weights, X[i]))
                error = y[i] - prediction
                gradient += error * X[i]
            self.weights += self.learning_rate * gradient / len(y)
```

### Step 6: Running the Perceptron on Example Data
We train the perceptron using a dataset and check its predictions.
```python
if __name__ == "__main__":
    data = pd.DataFrame({
        'x1': [1, 2, 3, 4, 5],
        'x2': [2, 3, 4, 5, 6],
        'y': [2.2, 2.8, 3.6, 4.5, 5.1]
    })
    X = data[['x1', 'x2']].values
    y = data['y'].values
    
    perceptron = PerceptronGD(input_size=2, activation='linear')
    perceptron.train(X, y)
    
    print("Predictions:")
    for sample in X:
        print(f"Input: {sample}, Predicted: {perceptron.predict(sample)}")
```

---

## What This Code Does
1. **Creates a Perceptron** with two inputs.
2. **Trains it** using Gradient Descent on example data.
3. **Tests it** by predicting outputs for new inputs.

---

## Summary
âœ… A perceptron is a simple machine learning model.
âœ… It learns using **Gradient Descent** to improve over time.
âœ… It uses an activation function to decide outputs.
âœ… Our example trains the perceptron to predict values.

This is a basic version, but the idea is used in deep learning and AI!

Would you like README files for the last implementation as well? ðŸ˜Š

